# -*- coding: utf-8 -*-
"""CreditCardDataMining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13tJJgxWwtKOUCyI8-IxRk6u1t92NDG3V

#Bu dataset, müşterilerin satın almadıkları ürünler için ücret alınmaması adına kredi kartı şirketlerinin hileli kredi kartı işlemlerini fark etmeleri istenir.:
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/creditcard.csv")

"""#Data'yı önizleyelim

#Eylül 2013'te Avrupalı kart sahipleri tarafından kredi kartlarıyla yapılan işlemleri içerir.
"""

data

"""#Hangi Veri Tipleri var"""

data.info()

"""#Sütun Sayısı

#284.807 işlemden 492'si dolandırıcılık olduğu iki gün içerisinde gerçekleien işlemleri sunuyor. Bu yüzden veri kümesi dengesiz ilerleyen satırlarda SMOTE işlemi gerçekleştireceğiz.
"""

data.shape

"""#Kayıp veri var mı ?"""

data.isnull().sum()

"""#Kaç tanesi normal kaç tanesi hileli işlem olduğunu yazar."""

len((data[(data['Amount'] < 0) & (data['Class'] == 1)]))

len((data[(data['Amount'] < 0) & (data['Class'] == 0)]))

data.drop(columns = 'Time', axis = 1)

data['Class'].value_counts()

data[data['Amount'] > 2000].groupby('Class').sum()['Amount']

(data["Class"].value_counts()/284807)*100

print("Class 1 için en yüksek Amount değeri:", data[data['Class'] == 1]['Amount'].max())
print("Class 1 için en düşük Amount değeri:", data[data['Class'] == 1]['Amount'].min())
print("Class 1 için ortalama Amount değeri:", data[data['Class'] == 1]['Amount'].mean())

print("Class 1 için en yüksek Amount değeri:", data[data['Class'] == 0]['Amount'].max())
print("Class 1 için en düşük Amount değeri:", data[data['Class'] == 0]['Amount'].min())
print("Class 1 için ortalama Amount değeri:", data[data['Class'] == 0]['Amount'].mean())

dataTemp = data[data['Amount'] != 0]

print("Class 1 için en yüksek Amount değeri:", dataTemp[dataTemp['Class'] == 1]['Amount'].max())
print("Class 1 için en düşük Amount değeri:", dataTemp[dataTemp['Class'] == 1]['Amount'].min())
print("Class 1 için ortalama Amount değeri:", dataTemp[dataTemp['Class'] == 1]['Amount'].mean())

print("Class 1 için en yüksek Amount değeri:", dataTemp[dataTemp['Class'] == 0]['Amount'].max())
print("Class 1 için en düşük Amount değeri:", dataTemp[dataTemp['Class'] == 0]['Amount'].min())
print("Class 1 için ortalama Amount değeri:", dataTemp[dataTemp['Class'] == 0]['Amount'].mean())

class_1_original = data[data['Class'] == 1]['Amount']
class_0_original = data[data['Class'] == 0]['Amount']
class_1_filtered = dataTemp[dataTemp['Class'] == 1]['Amount']
class_0_filtered = dataTemp[dataTemp['Class'] == 0]['Amount']
plt.figure(figsize=(15, 8))
plt.subplot(2, 2, 1)
sns.lineplot(x=class_1_original.index, y=class_1_original.values, label='Original Class 1')
sns.lineplot(x=class_1_filtered.index, y=class_1_filtered.values, label='Filtered Class 1')
plt.title('Class 1 - Amount: Line Plot')
plt.xlabel('Index')
plt.ylabel('Amount')
plt.legend()
plt.subplot(2, 2, 2)
sns.boxplot(x='Class', y='Amount', data=data, showfliers=False)
plt.title('Original Data - Box Plot')

plt.subplot(2, 2, 3)
sns.boxplot(x='Class', y='Amount', data=dataTemp, showfliers=False)
plt.title('Filtered Data (Amount != 0) - Box Plot')

plt.subplot(2, 2, 4)
sns.kdeplot(class_1_original, label='Original Class 1', fill=True)
sns.kdeplot(class_1_filtered, label='Filtered Class 1', fill=True)
plt.title('Class 1 - Amount: Density Plot')
plt.xlabel('Amount')
plt.ylabel('Density')
plt.legend()

plt.tight_layout()
plt.show()

print("Hilesiz işlemdeki 0 sayısı : ",len(data[(data['Amount'] == 0) & (data['Class'] == 0)]))
zeroP = len(data[(data['Amount'] == 0) & (data['Class'] == 0)]) / len(data[(data['Class'] == 0)])
print("Hilesiz işlemde sıfırın oranı : ", zeroP)

print("Hilesiz işlemdeki 0 sayısı : ",len(data[(data['Amount'] == 0) & (data['Class'] == 1)]))
zeroPFraud = len(data[(data['Amount'] == 0) & (data['Class'] == 1)]) / len(data[(data['Class'] == 1)])
print("Hileli işlemde sıfırın oranı : ", zeroPFraud)

zeroPNorm = (zeroP - data[data['Class'] == 0]['Amount'].min()) / (data[data['Class'] == 0]['Amount'].max() - data[data['Class'] == 0]['Amount'].min())
zeroPFraudNorm = (zeroPFraud - data[data['Class'] == 1]['Amount'].min()) / (data[data['Class'] == 1]['Amount'].max() - data[data['Class'] == 1].min())

"""#Amaç
#Normal işlemleri ve hileli işlemleri ayrı ayrı datalara atayalım.
"""

normalProcess = data[data.Class == 0]
fraudProcess = data[data.Class == 1]

fraudProcess.shape[0]

normalProcess.shape[0]

fraud_data = dataTemp[dataTemp['Class'] == 1]
non_fraud_data = dataTemp[dataTemp['Class'] == 0]

plt.figure(figsize = (12, 6))
sns.boxplot(x='Class', y='Amount', data = data, palette = ['blue', 'red'])

plt.xlabel('Dolandırıcılık Durumu')
plt.ylabel('Amount (log scale)')
plt.yscale('log')
plt.title('Amount Değerlerine Göre Dolandırıcılık ve Dolandırıcılık Olmayan İşlemler')

plt.xticks(ticks=[0, 1], labels=['Dolandırıcılık Yok', 'Dolandırıcılık Var'])
plt.show()

"""#Time değerinin bir önemi yok çünkü kullanılmıyor.

#İstatiksel ve Matematiksel özet
"""

fraudProcess.Amount.describe()

"""#Her iki işlemi'de karşılaştıralım."""

data.groupby('Class').mean()

"""#İki etiket arasında benzer dağılımı içeren örnek bir veri kümesi oluşturalım.


"""

normalProcessSample = normalProcess.sample(n = fraudProcess.shape[0])

"""#İki etiketi barındıran dataframeleri birleştirelim."""

data = pd.concat([normalProcessSample, fraudProcess], axis = 0)

data

data.tail()

data['Class'].value_counts()

data.groupby('Class').mean()

x = data.drop(columns = 'Class', axis = 1)
y = data['Class']

scaler = StandardScaler()
x = scaler.fit_transform(x)

x.shape

y.shape

"""#Test ve eğitim verilerini bölelim.

#Stratify kullanımı, sınıfların dağılımını korur, bu özellik eğitim ve test verilerindeki sınıf oranları, orijinal veri ile aynı şekilde düzenler.
"""

xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, stratify = y, random_state = 42)

smote = SMOTE(random_state=42)
xTrain, yTrain = smote.fit_resample(xTrain, yTrain)

print(x.shape, xTrain.shape, xTest.shape)

"""#Eğitime başlayalım

#Bu eğitimi Logistic Regression Modeli ile yapacağız.
"""

model = LogisticRegression()

model.fit(xTrain, yTrain)

"""#Model eğitimi sonuçları alımı."""

xTrainPred = model.predict(xTrain)
trainingDataAccuracy = accuracy_score(xTrainPred, yTrain)

print('Accuracy on Training Data = ', trainingDataAccuracy)

"""#Sonuçlara birde Test datası ile bakalım."""

xTestPred = model.predict(xTest)
testDataAccuracy = accuracy_score(xTestPred, yTest)

print('Accuracy on Test Data = ', testDataAccuracy)

num_features_in_model = model.coef_.shape[1]
print("Modelin eğitildiği özellik sayısı:", num_features_in_model)

print("Test veri setinin boyutu:", yTest.values.shape)

yTestPred = model.predict(xTest)

def evaluate_model(x_test, y_test, model):
    y_test_pred = model.predict(x_test)
    y_test_cl_report = classification_report(y_test, y_test_pred, target_names=['No Fraud', 'Fraud'])
    print("_" * 100)
    print("TEST MODEL CLASSIFICATION REPORT")
    testAccuracy = accuracy_score(y_test, y_test_pred)
    print("Test veri seti doğruluğu:", testAccuracy)
    print(y_test_cl_report)

    cm = confusion_matrix(y_test, y_test_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['Dolandırıcılık Yok', 'Dolandırıcılık Var'],
                yticklabels=['Dolandırıcılık Yok', 'Dolandırıcılık Var'])
    plt.xlabel('Tahmin Edilen Sınıf')
    plt.ylabel('Gerçek Sınıf')
    plt.title('Test Veri Seti Confusion Matrisi')
    plt.show()

model = SVC(random_state=42)
model.fit(xTrain, yTrain)
evaluate_model(xTest, yTest, model)

"""#Rastgele örnekleme ve öznitelik seçimi yapıldıktan sonra, her bir alt küme üzerinde bir karar ağacı oluşturulur. Karar ağaçları genellikle bölme (split) işlemleri ile veriyi sınıflandırır."""

model = RandomForestClassifier(random_state=42)
model.fit(xTrain, yTrain)
evaluate_model(xTest, yTest, model)

model = DecisionTreeClassifier(random_state=42)
model.fit(xTrain, yTrain)
evaluate_model(xTest, yTest, model)

model = LogisticRegression()
model.fit(xTrain, yTrain)
evaluate_model(xTest, yTest, model)

"""#GBClassifier, eXtreme Gradient Boosting (XGBoost) kütüphanesinin sınıflandırma için özel olarak tasarlanmış bir sınıflandırıcı (classifier) sınıfıdır. XGBoost, gradient boosting yöntemini kullanarak, çoklu karar ağaçları üzerinde toplu bir öğrenme modeli oluşturur. XGBoost, performansıyla bilinir ve birçok yarışma ve uygulama alanında başarıyla kullanılmıştır."""

from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(xTrain, yTrain)
evaluate_model(xTest, yTest, model)

